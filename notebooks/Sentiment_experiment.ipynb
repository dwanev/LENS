{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wtR1FQi0QdR6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(24)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(24)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import anchor\n",
    "from anchor import anchor_text\n",
    "from sklearn import preprocessing\n",
    "import sys; sys.path.insert(0, '../src/')\n",
    "import necsuf_word_level as nec_suf_repl\n",
    "import necsuf_tabular_text as nec_suf \n",
    "from nltk.corpus import stopwords   \n",
    "from sklearn.model_selection import train_test_split     \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  \n",
    "from tensorflow.keras.models import Sequential     \n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "from tensorflow.keras.models import load_model  \n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing and model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the procedure in https://github.com/hansmichaels/sentiment-analysis-IMDB-Review-using-LSTM/blob/master/sentiment_analysis.py.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xjBGZwNzKs_p",
    "outputId": "88445c55-14af-4a5b-b061-6b6d67a3cda9"
   },
   "outputs": [],
   "source": [
    "# This dataset is voluminous and therefore not included in src. Please download from https://github.com/hansmichaels/sentiment-analysis-IMDB-Review-using-LSTM and place in path.\n",
    "path = \"../datasets/\"\n",
    "data = pd.read_csv(path+'IMDB_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tYqIiWcOKs_q"
   },
   "outputs": [],
   "source": [
    "english_stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6tK9ykbmKs_r"
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    df = pd.read_csv(path+'IMDB_Dataset.csv')\n",
    "    x_data = df['review']       # Reviews/Input\n",
    "    y_data = df['sentiment']    # Sentiment/Output\n",
    "\n",
    "    # PRE-PROCESS REVIEW\n",
    "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
    "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
    "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
    "    \n",
    "    # ENCODE SENTIMENT -> 0 & 1\n",
    "    y_data = y_data.replace('positive', 1)\n",
    "    y_data = y_data.replace('negative', 0)\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "x_data, y_data = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mJ15-bIzKs_s"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8D2dNuxbKs_t"
   },
   "outputs": [],
   "source": [
    "def get_max_length():\n",
    "    review_length = []\n",
    "    for review in x_train:\n",
    "        review_length.append(len(review))\n",
    "\n",
    "    return int(np.ceil(np.mean(review_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-mVFF9zZKs_u"
   },
   "outputs": [],
   "source": [
    "# Encode review\n",
    "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
    "token.fit_on_texts(x_train)\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "x_test = token.texts_to_sequences(x_test)\n",
    "\n",
    "max_length = get_max_length()\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "total_words = len(token.word_index) + 1   # add 1 because of 0 padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and train model -- steps included for transperancy, but we load model from a saved location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nWP0l111Ks_v"
   },
   "outputs": [],
   "source": [
    "# # ARCHITECTURE\n",
    "# EMBED_DIM = 32\n",
    "# LSTM_OUT = 64\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\n",
    "# model.add(LSTM(LSTM_OUT))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mfESbnL9Ks_x"
   },
   "outputs": [],
   "source": [
    "# checkpoint = ModelCheckpoint(\n",
    "#     path+'models/LSTM.h5',\n",
    "#     monitor='accuracy',\n",
    "#     save_best_only=True,\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GSlbOH2nKs_x"
   },
   "outputs": [],
   "source": [
    "# model.fit(x_train, y_train, batch_size = 128, epochs = 5, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RhjX6Hnfcw7b"
   },
   "outputs": [],
   "source": [
    "# model.save(path+'models/final_sent_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "mwhbBeodKs_z"
   },
   "outputs": [],
   "source": [
    "# One could retrain the model above, we just load a pre-trained model via the procedure above\n",
    "loaded_model = load_model(path+'models/final_sent_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Fb7SMeKAKs_y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Prediction: 8703\n",
      "Wrong Prediction: 1297\n",
      "Accuracy: 87.03\n"
     ]
    }
   ],
   "source": [
    "# Test model. Our saved model should have a test accuracy of 87.03\n",
    "y_pred = (loaded_model.predict(x_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "true = 0\n",
    "for i, y in enumerate(y_test):\n",
    "    if y == y_pred[i]:\n",
    "        true += 1\n",
    "\n",
    "print('Correct Prediction: {}'.format(true))\n",
    "print('Wrong Prediction: {}'.format(len(y_pred) - true))\n",
    "print('Accuracy: {}'.format(true/len(y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LG5Fi7QKs_z"
   },
   "source": [
    "Helper functions: Tokenize raw input sentence, predict utility (predict_lr), utilities to find short wrongly predicted sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "bRB7NpoDKs_2"
   },
   "outputs": [],
   "source": [
    "def str_to_rep(review):\n",
    "    # Pre-process input\n",
    "    if isinstance(review, list):\n",
    "        review = review[0]\n",
    "    regex = re.compile(r'[^a-zA-Z\\s]')\n",
    "    review = regex.sub('', review)\n",
    "\n",
    "    words = review.split(' ')\n",
    "    filtered = [w for w in words if w not in english_stops]\n",
    "    filtered = ' '.join(filtered)\n",
    "    filtered = [filtered.lower()]\n",
    "\n",
    "    tokenize_words = token.texts_to_sequences(filtered)\n",
    "    tokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "    return tokenize_words\n",
    "\n",
    "def predict_lr(review, anchors=True):\n",
    "    if isinstance(review, list):\n",
    "        review = review[0]\n",
    "    regex = re.compile(r'[^a-zA-Z\\s]')\n",
    "    review = regex.sub('', review)\n",
    "\n",
    "    words = review.split(' ')\n",
    "    filtered = [w for w in words if w not in english_stops]\n",
    "    filtered = ' '.join(filtered)\n",
    "    filtered = [filtered.lower()]\n",
    "\n",
    "    tokenize_words = token.texts_to_sequences(filtered)\n",
    "    tokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "    pred = int((loaded_model.predict(tokenize_words)>=0.5)*1)\n",
    "    if anchors:\n",
    "        return np.array([pred])\n",
    "    else:\n",
    "        return pred\n",
    "    \n",
    "def sample_len(data, label, samp_len=10):\n",
    "    count = 0\n",
    "    ex = []\n",
    "    labels_ex = []\n",
    "    for (d, y) in zip(data, label):\n",
    "        if len(d)<samp_len:\n",
    "            count += 1\n",
    "            ex.append((d, y))\n",
    "    return ex\n",
    "\n",
    "def find_wrong_pred(short_sentences):\n",
    "    wrong_preds = []\n",
    "    count = 0\n",
    "    for sent, label in short_sentences:\n",
    "        tokenize_words = str_to_rep(\" \".join(sent))\n",
    "        pred = (loaded_model.predict(tokenize_words)>=0.5)*1.\n",
    "        if pred!=label:\n",
    "            wrong_preds.append((sent, pred, label))\n",
    "            count+=1\n",
    "    return wrong_preds\n",
    "\n",
    "short_sentences = sample_len(x_data, y_data)\n",
    "wrong_preds = find_wrong_pred(short_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: erroneous positive prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l63WFex0QUs1"
   },
   "source": [
    "Anchors ouptput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3dq-4pDhQaUK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read book forget movie\n",
      "Prediction: positive\n",
      "Anchor: read AND movie\n",
      "Precision: 0.94\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "explainer = anchor_text.AnchorText(nlp, ['negative', 'positive'], use_unk_distribution=True)\n",
    "np.random.seed(1)\n",
    "text = \" \".join(wrong_preds[1][0])\n",
    "print(text)\n",
    "pred = explainer.class_names[predict_lr([text])[0]]\n",
    "alternative =  explainer.class_names[1 - predict_lr([text])[0]]\n",
    "print('Prediction: %s' % pred)\n",
    "expl = explainer.explain_instance(text, predict_lr, threshold=0.9)\n",
    "\n",
    "print('Anchor: %s' % (' AND '.join(expl.names())))\n",
    "print('Precision: %.2f' % expl.precision())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-JCP_xBQr1c"
   },
   "source": [
    "LENS output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define inp and refs for our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \" \".join(wrong_preds[1][0])\n",
    "f_inp = predict_lr([inp])\n",
    "\n",
    "sent_dict = {i: word for (i, word) in enumerate(inp.split(\" \"))}\n",
    "sent_dict.update({len(inp.split(\" \")): '1'})\n",
    "\n",
    "ref_dict = {i: 'PLATE' for (i, _) in enumerate(inp.split(\" \"))}\n",
    "ref_dict.update({len(inp.split(\" \")): '0'})\n",
    "df_raw = pd.DataFrame([sent_dict, ref_dict])\n",
    "\n",
    "refs_ex = df_raw.loc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce D, find minimal sufficient factors in R2I, compute cumulative probability of necessity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CF_r2i = nec_suf_repl.create_CF_unk_text(df_raw.loc[0], df_raw.loc[1], predict_lr, r2i=True, datatype='Text')\n",
    "CF_i2r = nec_suf_repl.create_CF_unk_text(df_raw.loc[0], df_raw.loc[1], predict_lr, r2i=False, datatype='Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "Sufficiency R2I\n",
      "##################\n",
      "--------------------\n",
      "cumulative nec. score:  1.0\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subset</th>\n",
       "      <th>degree</th>\n",
       "      <th>string</th>\n",
       "      <th>cardinality</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0, 2, 3]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0   read, 2 forget, 3  movie</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subset  degree                        string  cardinality  cost\n",
       "13  [0, 2, 3]     1.0  0   read, 2 forget, 3  movie            3   NaN"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"##################\")\n",
    "print(\"Sufficiency R2I\")\n",
    "print(\"##################\")\n",
    "\n",
    "CF_df_deg_r2i = nec_suf_repl.deg_nec_suff(CF_r2i, df_raw.loc[0], f_inp, r2i=True)\n",
    "sub_df_filtered = nec_suf.filter_by_degree_and_overalp(CF_df_deg_r2i, degree_thresh=0.0, subset_max_num=10)\n",
    "print(\"--------------------\")\n",
    "print(\"cumulative nec. score: \", nec_suf.recall_nec_score(CF_r2i, sub_df_filtered, f_inp))\n",
    "print(\"--------------------\")\n",
    "sub_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "Sufficiency I2R\n",
      "##################\n",
      "--------------------\n",
      "cumulative nec. score:  1.0\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subset</th>\n",
       "      <th>degree</th>\n",
       "      <th>string</th>\n",
       "      <th>cardinality</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0 read</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2 forget</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[3]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3 movie</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subset  degree    string  cardinality  cost\n",
       "1    [0]     1.0    0 read            1   NaN\n",
       "3    [2]     1.0  2 forget            1   NaN\n",
       "4    [3]     1.0   3 movie            1   NaN"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"##################\")\n",
    "print(\"Sufficiency I2R\")\n",
    "print(\"##################\")\n",
    "\n",
    "CF_df_deg_i2r = nec_suf_repl.deg_nec_suff(CF_i2r, df_raw.loc[0], f_inp, r2i=False)\n",
    "sub_df_filtered_i2r = nec_suf.filter_by_degree_and_overalp(CF_df_deg_i2r, degree_thresh=0.9, subset_max_num=10)\n",
    "\n",
    "print(\"--------------------\")\n",
    "print(\"cumulative nec. score: \", nec_suf.recall_nec_score(CF_i2r, sub_df_filtered_i2r, f_inp, r2i=False))\n",
    "print(\"--------------------\")\n",
    "sub_df_filtered_i2r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Correct prediction (bonus example, not in paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: negative\n",
      "Anchor: terrible AND this\n",
      "Precision: 1.00\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "explainer = anchor_text.AnchorText(nlp, ['negative', 'positive'], use_unk_distribution=True)\n",
    "np.random.seed(1)\n",
    "text = \" \".join(short_sentences[4][0])\n",
    "pred = explainer.class_names[predict_lr([text])[0]]\n",
    "alternative =  explainer.class_names[1 - predict_lr([text])[0]]\n",
    "print('Prediction: %s' % pred)\n",
    "expl = explainer.explain_instance(text, predict_lr, threshold=0.9)\n",
    "\n",
    "print('Anchor: %s' % (' AND '.join(expl.names())))\n",
    "print('Precision: %.2f' % expl.precision())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define inp and refs for our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \" \".join(short_sentences[4][0])\n",
    "f_inp = predict_lr([inp])\n",
    "\n",
    "sent_dict = {i: word for (i, word) in enumerate(inp.split(\" \"))}\n",
    "sent_dict.update({len(inp.split(\" \")): '1'})\n",
    "\n",
    "ref_dict = {i: 'UNK' for (i, _) in enumerate(inp.split(\" \"))}\n",
    "ref_dict.update({len(inp.split(\" \")): '0'})\n",
    "df_raw = pd.DataFrame([sent_dict, ref_dict])\n",
    "\n",
    "refs_ex = df_raw.loc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce D, find minimal sufficient factors in R2I, compute cumulative probability of necessity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "CF_r2i = nec_suf_repl.create_CF_unk_text(df_raw.loc[0], df_raw.loc[1], predict_lr, r2i=True, datatype='Text')\n",
    "CF_i2r = nec_suf_repl.create_CF_unk_text(df_raw.loc[0], df_raw.loc[1], predict_lr, r2i=False, datatype='Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "Sufficiency R2I\n",
      "##################\n",
      "--------------------\n",
      "cumulative nec. score:  1.0\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subset</th>\n",
       "      <th>degree</th>\n",
       "      <th>string</th>\n",
       "      <th>cardinality</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2 terrible</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subset  degree      string  cardinality  cost\n",
       "3    [2]     1.0  2 terrible            1   NaN"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"##################\")\n",
    "print(\"Sufficiency R2I\")\n",
    "print(\"##################\")\n",
    "\n",
    "CF_df_deg_r2i = nec_suf_repl.deg_nec_suff(CF_r2i, df_raw.loc[0], f_inp, r2i=True)\n",
    "sub_df_filtered = nec_suf.filter_by_degree_and_overalp(CF_df_deg_r2i, degree_thresh=0.0, subset_max_num=10)\n",
    "print(\"--------------------\")\n",
    "print(\"cumulative nec. score: \", nec_suf.recall_nec_score(CF_r2i, sub_df_filtered, f_inp))\n",
    "print(\"--------------------\")\n",
    "sub_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "Sufficiency I2R\n",
      "##################\n",
      "--------------------\n",
      "cumulative nec. score:  1.0\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subset</th>\n",
       "      <th>degree</th>\n",
       "      <th>string</th>\n",
       "      <th>cardinality</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2 terrible</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subset  degree      string  cardinality  cost\n",
       "3    [2]     1.0  2 terrible            1   NaN"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"##################\")\n",
    "print(\"Sufficiency I2R\")\n",
    "print(\"##################\")\n",
    "\n",
    "CF_df_deg_i2r = nec_suf_repl.deg_nec_suff(CF_i2r, df_raw.loc[0], f_inp, r2i=False)\n",
    "sub_df_filtered_i2r = nec_suf.filter_by_degree_and_overalp(CF_df_deg_i2r, degree_thresh=0.9, subset_max_num=10)\n",
    "\n",
    "print(\"--------------------\")\n",
    "print(\"cumulative nec. score: \", nec_suf.recall_nec_score(CF_i2r, sub_df_filtered_i2r, f_inp, r2i=False))\n",
    "print(\"--------------------\")\n",
    "sub_df_filtered_i2r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: brittle prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: negative\n",
      "Anchor: choose AND better AND even AND you AND paul AND verhoeven\n",
      "Precision: 0.95\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "explainer = anchor_text.AnchorText(nlp, ['negative', 'positive'], use_unk_distribution=True)\n",
    "np.random.seed(1)\n",
    "text = \" \".join(short_sentences[5][0])\n",
    "pred = explainer.class_names[predict_lr([text])[0]]\n",
    "alternative =  explainer.class_names[1 - predict_lr([text])[0]]\n",
    "print('Prediction: %s' % pred)\n",
    "expl = explainer.explain_instance(text, predict_lr, threshold=0.9)\n",
    "\n",
    "print('Anchor: %s' % (' AND '.join(expl.names())))\n",
    "print('Precision: %.2f' % expl.precision())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define inp and refs for our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \" \".join(short_sentences[5][0])\n",
    "f_inp = predict_lr([inp])\n",
    "\n",
    "sent_dict = {i: word for (i, word) in enumerate(inp.split(\" \"))}\n",
    "sent_dict.update({len(inp.split(\" \")): '1'})\n",
    "\n",
    "ref_dict = {i: 'UNK' for (i, _) in enumerate(inp.split(\" \"))}\n",
    "ref_dict.update({len(inp.split(\" \")): '0'})\n",
    "df_raw = pd.DataFrame([sent_dict, ref_dict])\n",
    "\n",
    "refs_ex = df_raw.loc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce D, find minimal sufficient factors in R2I, compute cumulative probability of necessity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "CF_r2i = nec_suf_repl.create_CF_unk_text(df_raw.loc[0], df_raw.loc[1], predict_lr, r2i=True, datatype='Text')\n",
    "CF_i2r = nec_suf_repl.create_CF_unk_text(df_raw.loc[0], df_raw.loc[1], predict_lr, r2i=False, datatype='Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "Sufficiency R2I\n",
      "##################\n",
      "--------------------\n",
      "cumulative nec. score:  1.0\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subset</th>\n",
       "      <th>degree</th>\n",
       "      <th>string</th>\n",
       "      <th>cardinality</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2 choose</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5 even</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subset  degree    string  cardinality  cost\n",
       "3    [2]     1.0  2 choose            1   NaN\n",
       "6    [5]     1.0    5 even            1   NaN"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"##################\")\n",
    "print(\"Sufficiency R2I\")\n",
    "print(\"##################\")\n",
    "\n",
    "CF_df_deg_r2i = nec_suf_repl.deg_nec_suff(CF_r2i, df_raw.loc[0], f_inp, r2i=True)\n",
    "sub_df_filtered = nec_suf.filter_by_degree_and_overalp(CF_df_deg_r2i, degree_thresh=0.0, subset_max_num=10)\n",
    "print(\"--------------------\")\n",
    "print(\"cumulative nec. score: \", nec_suf.recall_nec_score(CF_r2i, sub_df_filtered, f_inp))\n",
    "print(\"--------------------\")\n",
    "sub_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "Sufficiency I2R\n",
      "##################\n",
      "--------------------\n",
      "cumulative nec. score:  1.0\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subset</th>\n",
       "      <th>degree</th>\n",
       "      <th>string</th>\n",
       "      <th>cardinality</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1 better</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2 choose</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[3]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3 paul</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5 even</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subset  degree    string  cardinality  cost\n",
       "2    [1]     1.0  1 better            1   NaN\n",
       "3    [2]     1.0  2 choose            1   NaN\n",
       "4    [3]     1.0    3 paul            1   NaN\n",
       "6    [5]     1.0    5 even            1   NaN"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"##################\")\n",
    "print(\"Sufficiency I2R\")\n",
    "print(\"##################\")\n",
    "\n",
    "CF_df_deg_i2r = nec_suf_repl.deg_nec_suff(CF_i2r, df_raw.loc[0], f_inp, r2i=False)\n",
    "sub_df_filtered_i2r = nec_suf.filter_by_degree_and_overalp(CF_df_deg_i2r, degree_thresh=0.9, subset_max_num=10)\n",
    "\n",
    "print(\"--------------------\")\n",
    "print(\"cumulative nec. score: \", nec_suf.recall_nec_score(CF_i2r, sub_df_filtered_i2r, f_inp, r2i=False))\n",
    "print(\"--------------------\")\n",
    "sub_df_filtered_i2r"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sentiment_analysis.py.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
